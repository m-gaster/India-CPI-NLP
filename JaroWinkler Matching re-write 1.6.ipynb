{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replica of Mikhael's Code Nad Version November 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###               TABLE OF CONTENTS\n",
    "#### DATA CLEANING - (70-215)\n",
    "#### UNITS - (220-860)\n",
    "    PARSING ACTUAL UNITS direct from string - (220-400)\n",
    "    PARSING FROM GIVEN UNIT MEASUREMENTS - (400-470)\n",
    "    PARSING UNITS BY FREQUENCY - (475-640)\n",
    "    UNIT CONVERSION - (650-830)\n",
    "    COMPILING FINAL UNIT LIST - (840-860)\n",
    "#### MATCHING ENTRIES\n",
    "    CLEANING SPEC CODES - (870-940)\n",
    "    JARO-WINKLER MATCHING - (940-1060)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in /opt/conda/lib/python3.7/site-packages (4.1.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "##Importing Packages\n",
    "import textdistance\n",
    "from nltk import pos_tag\n",
    "from nltk import FreqDist\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import csv\n",
    "import sys\n",
    "import pandas as pd\n",
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "## Nad: Had to do this to make nltk work\n",
    "##import nltk\n",
    "## nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing and setting up data.\n",
    "\n",
    "#create 4 lists to hold information\n",
    "Itemcode = []\n",
    "Spec_code = []\n",
    "Unit = []\n",
    "Price = []\n",
    "##Function to call in itemcode, spcification and Unit in\n",
    "def array_filler(line): \n",
    "    for jt in line:\n",
    "        Itemcode.append(jt[1])\n",
    "        Spec_code.append(jt[2])\n",
    "        Unit.append(jt[3])\n",
    "        Price.append(jt[4])\n",
    "    # remove titles from the first entry in the list\n",
    "    del Itemcode[0]\n",
    "    del Spec_code[0]\n",
    "    del Unit[0]\n",
    "    del Price[0]\n",
    "\n",
    "with open(r\"Sorted Compressed Data 4 - Local Names Mapped.csv\") as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    array_filler(reader)\n",
    "\n",
    "\"\"\"\n",
    "Capitalizing all words in the strings\n",
    "\n",
    "\"\"\"\n",
    "## STANDARDISATION IS NECESSARY TO CARRY OUT OPERATIONS\n",
    "def Upperer(array):\n",
    "    for n in range(len(array)):\n",
    "        array[n] = array[n].upper()\n",
    "\n",
    "Upperer(Spec_code)\n",
    "Upperer(Itemcode)\n",
    "## Why not Upper for Units??\n",
    "\n",
    "\n",
    "\n",
    "itemcode_dict = {}\n",
    "itemcode_set = set()\n",
    "\n",
    "for item_category in Itemcode:\n",
    "    itemcode_set.add(item_category)\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    itemcode_dict[item_category] = ''\n",
    "    \n",
    "og_spec_code = Spec_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beginning of DATA CLEANING\n",
    " a. delete punctuation and some useless words. Spelled-out numbers will be replaced. For exampple, 'ONE'-->'1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 20 GAUGE 3/6 X 20 INCHES AMAR STEEL LOCAL MADE\n",
      "768 AP STATE HANDLOOM CO OPERATIVE BED SHEET PRINTED 60 X 90 2/40 X 2/40 KARWAN SOCIETY WITH 2 PILLOW COVERS\n",
      "768 AP STATE HANDLOOM CO OPERATIVE BED SHEET PRINTED 60 X 90 2/40 X 2/40 KARWAN SOCIETY WITH 2 PILLOW COVERS\n",
      "770 APCO SHOWROOM BED SPREAD PRINTED 60 INCHES X 90 INCHES 2/40 X 2/20 KARWAN SOCIETY WITH 2 PILLOW COVER\n",
      "770 APCO SHOWROOM BED SPREAD PRINTED 60 INCHES X 90 INCHES 2/40 X 2/20 KARWAN SOCIETY WITH 2 PILLOW COVER\n",
      "772 APCO SHOWROOM KARIM NAGAR SOCIETY COLOURED DESIGNED 60 INCHES X 90 INCHES 2/20 X 2/20\n",
      "772 APCO SHOWROOM KARIM NAGAR SOCIETY COLOURED DESIGNED 60 INCHES X 90 INCHES 2/20 X 2/20\n",
      "840 DCM SHOWROOM DCM NO 2/187/5151 COTTON WITH PILLOW 54 INCHES X 90 INCHES 135 X 225 CMS\n",
      "840 DCM SHOWROOM DCM NO 2/187/5151 COTTON WITH PILLOW 54 INCHES X 90 INCHES 135 X 225 CMS\n",
      "843 DCM SHOWROOM SINGLE SIZE 54 INCHES X 90 INCHES PRINTED SET WITH PILLOW COVER 7/537/2001\n",
      "843 DCM SHOWROOM SINGLE SIZE 54 INCHES X 90 INCHES PRINTED SET WITH PILLOW COVER 7/537/2001\n",
      "884 NTC SHOWROOM WHITE NO 86416 60/62 BED SHEET BLEACHED 150 CM X 225 CMS\n",
      "901 PRIYADARSHNI HANDLOOM COTTON PRINTED HANDLOOMS 2/20 S 48 INCHES X 90 INCHES SIZE\n",
      "902 PRIYADARSHNI HANDLOOM COTTON PRINTED HANDLOOMS 2/20 S 48 INCHES X 90 INCHES SIZE\n",
      "903 PRIYADARSHNI HANDLOOM COTTON COLOURED HANDLOOMS 2/20 S 48 INCHES X 90 INCHES SIZE\n",
      "1054 HERO JET 22 INCHES GENTS WITH SEAT AND 1/2 CHAIN COVER BELL CARRIER AND STAND WITH FITTING CHARGES\n",
      "1494 DCM SHOWROOM DCM NO 25/ 89/ 2504 WOLLEN CHECK SIZE 60 INCHES X 90 INCHES\n",
      "1494 DCM SHOWROOM DCM NO 25/ 89/ 2504 WOLLEN CHECK SIZE 60 INCHES X 90 INCHES\n",
      "1613 PRIYADARSHNI HANDLOOM 100% POLYSTER 2/60 S X 2/60 S PLAIN COLOURED WIDTH 90 CMS\n",
      "1613 PRIYADARSHNI HANDLOOM 100% POLYSTER 2/60 S X 2/60 S PLAIN COLOURED WIDTH 90 CMS\n",
      "1674 OTHER THAN MILL CHOICE VASHI LISSY COTTON SIZE 34/85 VGP PRODUCT\n",
      "2486 KURCHI IRON PIPE 7/8 DIAMETER 20 GAUAGE FOLDING PLASTIC STRIP\n",
      "2533 GENTS D NO 876/3102 BATA JUBILEE\n",
      "2537 GENTS JUBLEE 20/21 876 3020\n",
      "2581 LADIES BATA 876/5905 SIZE 3 7\n",
      "2642 II GENTS JUBLEE D NO 876/3002 SIZE 6 10\n",
      "3930 18/20 GUAGE WITH NYLON STRIPS SIZE 6 FEET X 4 FEET LOCAL\n",
      "3973 STEEL FRAME NYLON STRIPS 11/4 INCHES PIPE SIZE 6 FEET X 3 FEET LOCAL\n",
      "4067 SANTARA PROOF 50 DGREE 28.55% V/V OBS 1/1 POUCH\n",
      "4466 DCM SHOWROOM DCM 9/184/7003 SUPER FINE BLEACHED 115 CM X 5 METRE\n",
      "4466 DCM SHOWROOM DCM 9/184/7003 SUPER FINE BLEACHED 115 CM X 5 METRE\n",
      "4503 PONTEX HAND LOOM SHOW ROOM NO 3110 100/80 COUNT SIZE 127 CMS X 3.6 MTRS\n",
      "5353 HILSA WHOLE MEDIUM SIZE 1 1/2 KG FRESH\n",
      "5385 RAHU FRESH MEDIUM SIZE 1 1/2 KG CUT PIECES\n",
      "5386 RAHU FRESH MEDIUM SIZE 1.1/2 KG CUT PIECES\n",
      "5389 RAHU 'POONA' FRESH MEDIUM IN CUT PIECES 1 1/2 KG SIZE\n",
      "5393 RAHU POONA FRESH MEDIUM IN CUT PIECES WEIGHT 1 1/2 KG\n",
      "5490 CAULIFLOWERS \"POOVU\" MALLI JASMINE STRINGED 1 1/2 FOOT LONG\n",
      "5519 CORIANDERGE MALA STRINGED 1/4 MT\n",
      "5647 WBHPD 100% COTTON WIDTH 75 CMS LENGTH 1.25 MTS WARP X WEFT COUNT 17 NF X 17NF/240 S\n",
      "5660 BOMBAY DYEING SHOWROOM SETTER FULL SLEVES RNS/85 CM\n",
      "5724 OTHER THAN MILL SONU WHITE 85 CMS P 3/BAPI RNS\n",
      "7335 DCM SHOWROOM DCM WINTER DELUXE SPUN AND SPUN YARN NO 6/ 323/ 0011 POLY 65% VISCOSE 35% WIDTH 90 CM\n",
      "7335 DCM SHOWROOM DCM WINTER DELUXE SPUN AND SPUN YARN NO 6/ 323/ 0011 POLY 65% VISCOSE 35% WIDTH 90 CM\n",
      "7353 NTC SHOWROOM NTC TATA ROSE NO 86093/ 2394 PRINTED POLY 67% COTTON 33% WIDTH 89 CM\n",
      "7354 NTC SHOWROOM NTC TATA ROSE NO 86093/ 2394 PRINTED POLYSTER 67% COTTON 33% WIDTH 89 CM\n",
      "7355 NTC SHOWROOM NTC TATA ROSE NO 86093/ 2394 PRINTED POLYSTER 67% COTTON 33% WIDTH 89 CM\n",
      "7385 OTHER THAN MILL BOMBAY COTTON MILLS BOMBAY SUPER DELUX RANGE 'RANG' SUPER QUALITY WIDTH 89/91 CMS POLY 65/67% VISCOSE 35/33%\n",
      "7385 OTHER THAN MILL BOMBAY COTTON MILLS BOMBAY SUPER DELUX RANGE 'RANG' SUPER QUALITY WIDTH 89/91 CMS POLY 65/67% VISCOSE 35/33%\n",
      "7385 OTHER THAN MILL BOMBAY COTTON MILLS BOMBAY SUPER DELUX RANGE 'RANG' SUPER QUALITY WIDTH 89/91 CMS POLY 65/67% VISCOSE 35/33%\n",
      "7860 DCM SHOWROOM DCM PAMBA 2/ 954 / 4054 UNBLEACHED WIDTH 150 CM\n",
      "7861 DCM SHOWROOM QUALITY NO 4/522/4222 WIDTH 89 CM\n",
      "7861 DCM SHOWROOM QUALITY NO 4/522/4222 WIDTH 89 CM\n",
      "7862 DCM SHOWROOM `LATHA' BLEACHED MEDIUM A 100% COTTON QUALITY NO 2/7382022 WIDTH 89 CMS\n",
      "7869 MAFATLAL SHOWROOM DHAN LAKSHAMI MILLS GAY BIRD PL 602 BLEACHED WIDTH 122 CM M 24/50\n",
      "7961 DCM SHOWROOM HINDON RIVOR MILL NO 4/911/506 SIZE 117 CM\n",
      "7961 DCM SHOWROOM HINDON RIVOR MILL NO 4/911/506 SIZE 117 CM\n",
      "8345 AMLOSAFE LSS/5 MG STRIP OF 10 TABS\n",
      "9033 SYRUP BRINIA 30 POWER 1/2 OUNCE BOTTLE FOR FEVER\n",
      "9034 SYRUP RASTIK 30 POWER 1/2 OUNCE BOTTLE FOR COUGH\n",
      "9930 BANGLES COLOURED SIZE 2 INCHES 21/2 INCHES FEROZABAD\n",
      "9975 GLASS BANGLES PLAIN COLOURED SIZE 2 1/4 INCHES\n",
      "9977 PLAIN GLASS BANGLES SIZE 2 1/4 INCHES FEROZABADI\n",
      "9978 PLAIN GLASS BANGLES SIZE 2 1/4 INCHES FEROZABAD\n",
      "9979 PLAIN GLASS BANGLES 2/1/4 INCHES FEROZABADI\n",
      "9979 PLAIN GLASS BANGLES 2/1/4 INCHES FEROZABADI\n",
      "9980 PLAIN GLASS BANGLES SIZE 2 1/4 INCHES FEROZABADI\n",
      "9981 PLAIN GLASS BANGLES SIZE 2 1/4 INCHES FEROZABADI\n",
      "10302 POTTUKADALA 1/2 KG PKT WITHOUT HUSK\n",
      "10584 \"KONIKA\" COLOUR V X 100 SIZE 36/135 MM\n",
      "10857 MONEY ORDER MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10858 MONEY ORDER CHARGES COMMISSION FOR RS 100/\n",
      "10859 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10860 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10861 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10862 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10863 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10864 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10865 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10866 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10867 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10868 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10869 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10870 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10871 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10872 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10873 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10874 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10875 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10876 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10877 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10878 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10879 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10880 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10881 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10882 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10883 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10884 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10885 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10886 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10887 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10888 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10889 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10890 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10891 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10892 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10893 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10894 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10895 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10896 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10897 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10898 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10899 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10900 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10901 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10902 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10903 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10904 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10905 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10906 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10907 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING M O FORM COST\n",
      "10908 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING FORM\n",
      "10909 MONEY ORDERFOR RS 100/ COMMISSION CHARGES\n",
      "10910 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10911 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10912 MONEY ORDER COMMISION CHARGES FOR RS 100/\n",
      "10913 MONEY ORDER COMMISION CHARGES FOR EVERY RS 100/\n",
      "10916 MONEY ORDER COMMISSION CHARGES FOR RS 100/\n",
      "10917 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING MF FORM COST\n",
      "10918 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING MO FORM COST\n",
      "10919 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10920 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10921 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10922 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10923 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10924 MONEY ORDER COMMISSION CHARGES FOR RS 100/ INCLUDING COST OF MO FORM\n",
      "10983 III MONEY ORDER COMMISSION CHARGES FOR RS 100/ WITH COST OF FORM\n",
      "10984 III MONEY ORDER COMMISSION CHARGES FOR RS 100/ WITH COST OF FORM\n",
      "11449 CLASS X SCIENCE STREAM 2HRS /6 DAYS/WEAK\n",
      "11450 CLASS X SCIENCE STREAM 2 HRS/2 DAYS/WEEK NAME OF THE INSTITUTION S S M COACHING CENTRE M K PLOT QUISOR LANE BENACHITTY\n",
      "11451 CLASS X SCIENCE STREAM 1 HRS /6 DAYS/WEAK\n",
      "11453 CLASS X SCIENCE STREAM 1 HRS/3 DAYS/WEAK NAME OF THE INSTITUTE 1 PRIME COACHING CENTRE SRIRAMPORE 2 IDEAL COACHING CENTRE BAUREA STATION ROAD 3 EXCEL COACHING CENTRE SHIBPUR\n",
      "11455 CLASS X SCIENCE STREAM 1 HRS/5 DAYS/WEEK NAME OF THE INSTITUTE ADVANCE ACADEMY BETWEEN ULBARI AND REHBARI PALTAN BAZAAR\n",
      "11456 CLASS XII SCIENCE STREAM 2 HRS/2 DAYS/WEEK NAME OF THE INSTITUTION S S M COACHING CENTRE M K PLOT QUISOR LANE BENACHITTY\n",
      "11457 CLASS XII SCIENCE STREAM 1 HRS /6 DAYS/WEAK\n",
      "11459 CLASS XII SCIENCE STREAM 2 HRS/1 DAYS/WEAK NAME OF THE INSTITUTE 1 PRIME COACHING CENTRE SRIRAMPORE 2 IDEAL COACHING CENTRE BAUREA STATION ROAD 3 EXCEL COACHING CENTRE SHIBPUR\n",
      "11460 CLASS XII SCIENCE STREAM 3 HRS/2 DAYS/WEEK NAME OF THE INSTITUE 1 PARA SONA RAJGANAGA HARTRON ROAD 2 BOSE TACNO COMMERCIAL INSTITUTE C 36 R III AMLADOHI MARKET\n",
      "11461 CLASS XII SCIENCE STREAM 1 HRS/5 DAYS/WEEK NAME OF THE INSTITUTE ADVANCE ACADEMY BETWEEN ULBARI AND REHBARI PALTAN BAZAAR\n",
      "11462 CLASS X SCIENCE STREAM 6 DAYS/1 HRS /WEEK\n",
      "11463 CLASS XII SCIENCSTREAM 6 DAYS/1 HRS /WEEK\n",
      "11584 NANDINI KHF DAIRY POUCH OF 500 ML/455 GMS\n",
      "12008 MC DOWELL BOTTLE OF 375 ML 1/2 BOTTLE\n",
      "12020 OLD MONK X  X  X RUM 375 ML 1/2 BOTTLE\n",
      "12560 CO OPTEX STORE SAMBALPUR HANDLOOM COTTON DESIGN NO 102 /457/111 121 CM X 5 MTR\n",
      "12560 CO OPTEX STORE SAMBALPUR HANDLOOM COTTON DESIGN NO 102 /457/111 121 CM X 5 MTR\n",
      "12561 DCM SHOWROOM 100% COTTON PRINTED SIZE 5.50 MTR X 117 CM NO 9/392/8224\n",
      "12561 DCM SHOWROOM 100% COTTON PRINTED SIZE 5.50 MTR X 117 CM NO 9/392/8224\n",
      "12562 DCM SHOWROOM D C M 9/392/755 114 115 CM X 5 METER\n",
      "12562 DCM SHOWROOM D C M 9/392/755 114 115 CM X 5 METER\n",
      "12564 DCM SHOWROOM NO 7/21/5111 100% COTTON PRINTED SIZE 114 CMS X 5.00 MTS\n",
      "12564 DCM SHOWROOM NO 7/21/5111 100% COTTON PRINTED SIZE 114 CMS X 5.00 MTS\n",
      "12593 UDYOG MANDIR CHURI BAZAR NO 5021/1151 100% COTTON 4.5 MTR X 115 CMS\n",
      "12596 WBHPD TANTUJA JAMDANI 100 % COTTON WIDTH 112 CMS LENGTH 5.5 METRES WARP X WEFT COUNT 2/17 X 2/17 NF\n",
      "12596 WBHPD TANTUJA JAMDANI 100 % COTTON WIDTH 112 CMS LENGTH 5.5 METRES WARP X WEFT COUNT 2/17 X 2/17 NF\n",
      "12643 BOMBAY DYEING SHOWROOM PTD POLY SAREE CHANDNI 5 MT X 112/114 CMS\n",
      "12653 DCM SHOWROOM DCM 9/392/7571 RAJRANI 114.5 CM X 5.25 MT\n",
      "12653 DCM SHOWROOM DCM 9/392/7571 RAJRANI 114.5 CM X 5.25 MT\n",
      "12667 MAFATLAL SHOWROOM SURAT POLYSTER 5.50 MTR WIDTH 112/114 CMS PRINTED DESIGN 100% POLYSTER\n",
      "12694 SIYA RAM HANDLOOM MYSORE SILK WIDTH 112/114 SURAT MADE 5.25 MTR\n",
      "12724 OTHER THAN MILL S 972647/37 DESIGN SX SOCEITY NO 710 SILK WEIGHT 369 GMS ZARI WEIGHT 25 GMS\n",
      "12729 WBHPD PRINTED 100% SILK 112 CMS X 5.5 METRES WARP X WEFT COUNT 20/22 D X 28/30 D\n",
      "12729 WBHPD PRINTED 100% SILK 112 CMS X 5.5 METRES WARP X WEFT COUNT 20/22 D X 28/30 D\n",
      "13047 DCM SHOWROOM DCM NO 25/97/5072 ARTI 90 X 195 CM PLAIN COLOURED\n",
      "13047 DCM SHOWROOM DCM NO 25/97/5072 ARTI 90 X 195 CM PLAIN COLOURED\n",
      "13071 OTHER THAN MILL WOOLLEN KASHMIRI SHAWL MEDIUM QUALITY 2 1/2 MTR X 45 CM\n",
      "13144 DCM SHOWROOM 100% COTTON NO 4/411/4974 WIDTH 90 CM\n",
      "13144 DCM SHOWROOM 100% COTTON NO 4/411/4974 WIDTH 90 CM\n",
      "13148 DCM SHOWROOM QUALITY NO 6/323/0011 WIDTH 90 CM WINTER DELUX\n",
      "13148 DCM SHOWROOM QUALITY NO 6/323/0011 WIDTH 90 CM WINTER DELUX\n",
      "13200 RAYMONDS SHOWROOM NO 80101223/2 100% COTTON WIDTH 90 CMS\n",
      "13288 DCM SHOWROOM BRITE STAR NO 6/87/0025 POLYSTER 52% VISCOSE 48% WIDTH 89 CMS\n",
      "13288 DCM SHOWROOM BRITE STAR NO 6/87/0025 POLYSTER 52% VISCOSE 48% WIDTH 89 CMS\n",
      "13289 DCM SHOWROOM D C M QUALITY NO 5/48/7901 WIDTH 90 CMS\n",
      "13289 DCM SHOWROOM D C M QUALITY NO 5/48/7901 WIDTH 90 CMS\n",
      "13290 DCM SHOWROOM DCM FANCY SHIRTING BLENDED FABRICS Q NO 7/19/2266 65% POLYSTER 35% VISCOSE WIDTH 89 CMS\n",
      "13290 DCM SHOWROOM DCM FANCY SHIRTING BLENDED FABRICS Q NO 7/19/2266 65% POLYSTER 35% VISCOSE WIDTH 89 CMS\n",
      "13291 DCM SHOWROOM QUALITY NO 5/48/8960 POLYSTER 70% VISCOSE 30% WIDTH 89 CMS\n",
      "13291 DCM SHOWROOM QUALITY NO 5/48/8960 POLYSTER 70% VISCOSE 30% WIDTH 89 CMS\n",
      "13292 DCM SHOWROOM QUALITY NO 5/49/3112 POLYSTER 67% VISCOSE 33% 1 MTR 89 CMS\n",
      "13292 DCM SHOWROOM QUALITY NO 5/49/3112 POLYSTER 67% VISCOSE 33% 1 MTR 89 CMS\n",
      "13306 MAFATLAL SHOWROOM 65% POLYSTER 35% VISCOSE WIDTH 89 CMS NO 21900/705/3739\n",
      "13306 MAFATLAL SHOWROOM 65% POLYSTER 35% VISCOSE WIDTH 89 CMS NO 21900/705/3739\n",
      "13333 NTC SHOWROOM NO 30709/30 000 67 33 WIDTH 89 CMS\n",
      "13347 PRIYADARSHNI HANDLOOM 100% POLYSTER 2/60SX2/60 S PLAIN COLOURED WIDTH 90 CM\n",
      "13347 PRIYADARSHNI HANDLOOM 100% POLYSTER 2/60SX2/60 S PLAIN COLOURED WIDTH 90 CM\n",
      "13352 RAYMONDS SHOWROOM NO 4388/23 67% POLY AND 33% VISCOSE WIDTH 90 CMS\n",
      "13370 VIMAL SHOWROOM POLY VISCOSE COLOURED PRINTED POLYSTER VISCOSE 70/30 WIDTH 90 CMS\n",
      "13398 BATA 824 6050/SIGNARE 6028 SIZE 6 10 UPPTER LEATHER RUBBER SOLE\n",
      "13403 BATA NO 824/6050 SUPER STRIDE SIZE 6 10\n",
      "13444 GENTS SUPER STRIDE 824/6028 MOULDED SOLE PVC SIZE 6 10\n",
      "14119 NOTE BOOK RULED PAPER 131/2 X 16 CMS HARD BOARD 172 PAGES\n",
      "15121 BPL 20 INCHES MODEL AZ 8553/94 R COLOURED\n",
      "15882 DCM SHOWROOM D C M NO 3/919/1355 KHAKI MEDIUM 74 CM WIDTH\n",
      "15882 DCM SHOWROOM D C M NO 3/919/1355 KHAKI MEDIUM 74 CM WIDTH\n",
      "15912 PRIYADARSHNI HANDLOOM DRILL COLOURED 2/30SX10 S 71 CM\n",
      "15923 ANKUR SARI COOL COMFORT Q NO 01/4381 01205 WIDTH 150 CMS 67% POLY 33% VISCOS\n",
      "15973 BOMBAY DYEING SHOWROOM FANCY POLY SUITING NO B 8008/B 9009 65% POLYSTER 35% VISCOSE WIDTH 147 CMS\n",
      "15975 BOMBAY DYEING SHOWROOM FANCY POLY SUITING NO B 8008/B 9009 POLYSTER 65% VISCOSE 35% WIDTH 147 CMS\n",
      "16002 DCM SHOWROOM DCM HINDON ATLAS NO 6/24/0085 BLENDED FABRIC POLYSTER 52% VISCOSE 48% WIDTH 137 CMS\n",
      "16002 DCM SHOWROOM DCM HINDON ATLAS NO 6/24/0085 BLENDED FABRIC POLYSTER 52% VISCOSE 48% WIDTH 137 CMS\n",
      "16003 DCM SHOWROOM DCM QTY NO 11/ 29/ 5614 WIDTH 142 CMS 63% POLYSTER 37% VISCOS DYED\n",
      "16003 DCM SHOWROOM DCM QTY NO 11/ 29/ 5614 WIDTH 142 CMS 63% POLYSTER 37% VISCOS DYED\n",
      "16007 DCM SHOWROOM DCM HINDON ATLAS NO 6/24/0085 BLENDED FABRIC POLY 52% VIS 48.1% WD 137 CMS\n",
      "16007 DCM SHOWROOM DCM HINDON ATLAS NO 6/24/0085 BLENDED FABRIC POLY 52% VIS 48.1% WD 137 CMS\n",
      "16027 MAFATLAL SHOWROOM MAFAT LAL FINNER POINT A POLYSTER 65% VISCOSE 35% WIDTH 147 CMS NO 6/330/0011\n",
      "16027 MAFATLAL SHOWROOM MAFAT LAL FINNER POINT A POLYSTER 65% VISCOSE 35% WIDTH 147 CMS NO 6/330/0011\n",
      "16041 NTC SHOWROOM DHARIWAL MILL NO 6/306/3 POLYSTER 65% VISCOSE 35% WIDTH 142 CMS\n",
      "16041 NTC SHOWROOM DHARIWAL MILL NO 6/306/3 POLYSTER 65% VISCOSE 35% WIDTH 142 CMS\n",
      "16068 PRIYADARSHNI HANDLOOM POLYESTER 2/40SX2/15 S SUITING WIDTH 137 COLOURED\n",
      "16068 PRIYADARSHNI HANDLOOM POLYESTER 2/40SX2/15 S SUITING WIDTH 137 COLOURED\n",
      "16077 RAYMONDS SHOWROOM RAYMONDS TROPICANA NO 01/457/01845 POL 67% VIS 33% PLAIN WIDTH 147 CMS\n",
      "16077 RAYMONDS SHOWROOM RAYMONDS TROPICANA NO 01/457/01845 POL 67% VIS 33% PLAIN WIDTH 147 CMS\n",
      "16092 VIMAL SHOWROOM POLY VISCOSE 75553 POLYSTER VISCOSE 67/33 WIDTH 147 CMS\n",
      "16234 CHHATRI 8 SPOKES SYNTHETIC CLOTH PLASTIC HANDLE SIZE 26/8\n",
      "16365 OTHER THAN MILL UNDERWEAR DORA ORDINARY 100% COTTON U SHAPE SIZE 90/95 CMS WITH ELASTIC\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(Spec_code):\n",
    "    if '/' in x:\n",
    "        for letter, y in enumerate(x):\n",
    "            if y == '/' and (x[letter - 1].isdigit() or x[letter + 1].isdigit()):\n",
    "                print(i,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(Spec_code):\n",
    "    Spec_code[i] = v.replace(',', ' ').replace('APPROXIMATELY', '').replace('APPROX', '').replace('PACKET',\n",
    "                                                                                                  'PKT').replace('PACK',\n",
    "                                                                                                                 'PKT').replace(\n",
    "        '@', ' ').replace('-', ' ').replace(' ONE', '1').replace(' TWO', '2').replace('THREE', '3').replace('FOUR',\n",
    "                                                                                                            '4').replace(\n",
    "        'FIVE', '5').replace('SIX', '6').replace('SEVEN', '7').replace(' EIGHT', '8').replace('NINE', '9').replace(\n",
    "        ' TEN', '10').replace('!', ' ').replace('[', ' ').replace(']', ' ').replace('{', ' ').replace('}', ' ').replace(\n",
    "        ';', ' ').replace(':', ' ').replace('?', ' ').replace('^', ' ').replace('&', ' and ').replace('*', ' ').replace(\n",
    "        '_', ' ').replace('~', ' ').replace('*', ' X ')\n",
    "\n",
    "## Notice that \"*\" is now being replaced by \" X \"    \n",
    "\n",
    "## DO WE NEED TO LOOK FOR MORE OF THESE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " b. create split Unit and Spec code arrays (split by word, e.g. one packet off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_Spec_code = [[]] * len(Spec_code)\n",
    "for index, x in enumerate(Spec_code):\n",
    "    split_Spec_code[index] = x.split()\n",
    "\n",
    "split_Unit_code = [[]] * len(Unit)\n",
    "for index, x in enumerate(Unit):\n",
    "    split_Unit_code[index] = x.split()\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. fixing 'milli litre' issue - sometimes the unit is given as \"Milli Litre\" (the space creates issues later on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(split_Unit_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        if word == 'Milli' and ind + 1 in range(len(line)) and line[ind + 1].lower() in (\n",
    "                'litre', 'liter', 'litres', 'liters', 'metre', 'meter', 'metres', 'meters'):\n",
    "            # print(str(line[ind:]) + ' ' + str(''.join(line[ind:ind + 2])))\n",
    "            line[ind] = ''.join(line[ind:ind + 2])\n",
    "            del line[ind + 1]\n",
    "\n",
    "for line in split_Spec_code:  # probably not necessary after above replacement, but won't hurt\n",
    "    for ind, word in enumerate(line):\n",
    "        for i, letter in enumerate(word):\n",
    "            if letter == ',' and i + 1 in range(len(word)) and word[i + 1] != ' ':\n",
    "                line[ind] = word[:i + 1] + ' ' + word[i + 1:]\n",
    "## CHECK FOR OTHER SIMILAR PROBLEMS IN MEASUREMENT UNITS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.separate (split) letters and numbers found in same word (e.g. \"1JAR\" --> \"1\", \"JAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in split_Spec_code:\n",
    "    for ind, word in enumerate(line):\n",
    "        if any(x.isalpha() for x in word) and any(y.isdigit() for y in word):\n",
    "            for index, letter in enumerate(word):\n",
    "                if word not in ('1ST', '2ND', '3RD', '4TH', '5TH', '6TH', '7TH', '8TH', '9TH', '10TH', '11TH', '12TH'):\n",
    "                    if index - 1 in range(len(word)) and letter.isdigit() and word[index - 1].isalpha():\n",
    "                        line[ind] = word[:index] + ' ' + word[index:]\n",
    "                    if index + 1 in range(len(word)) and letter.isdigit() and word[index + 1].isalpha():\n",
    "                        line[ind] = word[:index + 1] + ' ' + word[index + 1:]\n",
    "### SPLIT AGAIN TO TAKE CARE OF BUCKETS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e. REMOVES PERIODS FROM TEXT (if it's not a numerical decimal).(NOT doing this is KEY if you want to deal with abbreviations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        for i, letter in enumerate(word):\n",
    "            if letter == '.':\n",
    "                if not (i - 1 in range(len(word)) and i + 1 in range(len(word)) and (\n",
    "                        word[i - 1].isdigit() or word[i - 1] == ' ') and word[i + 1].isdigit()):\n",
    "                    split_Spec_code[index][ind] = word.replace('.', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f. Removes parentheses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        split_Spec_code[index][ind] = line[ind].replace('(', ' ').replace(')', ' ')\n",
    "        split_Spec_code[index][ind] = line[ind].replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "g. Map \"45X12\" --> \"45 X 12\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        if any(x == 'X' for x in word) and not any((x != 'X' and x.isalpha()) for x in word):\n",
    "            split_Spec_code[index][ind] = line[ind].replace('X', ' X ')\n",
    "            split_Spec_code[index][ind] = line[ind].replace('  ', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h. remove parentheses again. Honestly not sure why this is done twice but it might be necessary after splitting the \"X\"s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        split_Spec_code[index][ind] = line[ind].replace('(', ' ').replace(')', ' ')\n",
    "        split_Spec_code[index][ind] = line[ind].replace('  ', ' ')\n",
    "\n",
    "#remove double spaces\n",
    "New_Spec_code = [' '.join(x) for x in split_Spec_code]\n",
    "\n",
    "#remove double spaces again\n",
    "for index, line in enumerate(New_Spec_code):\n",
    "    New_Spec_code[index] = line.replace('  ', ' ')\n",
    "\n",
    "#create new list\n",
    "New_split_Spec_code = [[]] * len(Spec_code)\n",
    "for index, x in enumerate(New_Spec_code):\n",
    "    New_split_Spec_code[index] = x.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "i. This replaces symbols for Inches and feet with words: symbol \" is replaced with inches and symbol ' is replaced with feet. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(New_split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        if any(letter.isdigit() for i, letter in enumerate(word)) and (\n",
    "                word[len(word) - 1] == '\"' or word[len(word) - 1] == \"'\"):  # and (\"'\" in word or '\"' in word):\n",
    "            New_split_Spec_code[index][ind] = word.replace(\"'\", ' FEET ').replace('\"', ' INCHES ')\n",
    "\n",
    "int_new_Spec_code = [' '.join(x) for x in New_split_Spec_code]\n",
    "\n",
    "for index, line in enumerate(int_new_Spec_code):\n",
    "    int_new_Spec_code[index] = line.replace('  ', ' ')\n",
    "\n",
    "int_new_split_Spec_code = [[]] * len(Spec_code)\n",
    "for index, x in enumerate(int_new_Spec_code):\n",
    "    int_new_split_Spec_code[index] = x.split()\n",
    "\n",
    "for index, line in enumerate(int_new_split_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        if any(x == 'X' for x in word) and not any((x != 'X' and x.isalpha()) for x in word):\n",
    "            int_new_split_Spec_code[index][ind] = line[ind].replace('X', ' X ')\n",
    "            int_new_split_Spec_code[index][ind] = line[ind].replace('  ', ' ')\n",
    "\n",
    "new_Spec_code = [' '.join(x) for x in int_new_split_Spec_code]\n",
    "\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    new_Spec_code[index] = line.replace('  ', ' ')\n",
    "\n",
    "new_split_Spec_code = [[]] * len(Spec_code)\n",
    "for index, x in enumerate(new_Spec_code):\n",
    "    new_split_Spec_code[index] = x.split()\n",
    "\n",
    "new_split_Spec_code_for_removing_units = new_split_Spec_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### END OF DATA CLEANING SECTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING \n",
    "### Creating Lists\n",
    "#### 1. Actual Units\n",
    "a. Measurement words  such as 'Centimetres' direct from string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a set and dictionary of all possible ways to write a unit.\n",
    "\n",
    "unit_set = set()\n",
    "unit_set.update([\"GRAMS\", 'GMS.', 'GMS', 'GM.', 'GRAM', ' GR ', ' GR.', 'GM', 'GRM', 'GRMS', 'GRM.', 'GRMS.', 'GS',\n",
    "                 # 'GR.','GR', 'GS.',' GS','G.', conflated with gallons..?\n",
    "                 \"POUNDS\", ' LB', 'LB', 'LB.', ' LBS', 'LBS', 'LBS.', 'POUND',\n",
    "                 \"OUNCES\", \"OUNCES.\", 'OZ.', 'OZ', 'OUNCE', 'OUNCE.', 'OZS', 'OZS.',\n",
    "                 \"MILLILITERS\", 'MILLILITER', 'MILLILITRE', 'MILLILITRES', 'ML', 'ML.', 'MLS', 'MLS.', 'MIL', 'MIL.',\n",
    "                 'CC', 'CC.', 'CCS', 'CCS.',  # 'MILLI',\n",
    "                 \"MILLIMETER\", 'MILLIMETRE', 'MILLIMETERS', 'MILLIMITRES', 'MM', 'MMS', 'MM.', 'MMS.',\n",
    "                 \"LITERS\", 'LITER', 'LTR', 'LTRS', 'LTR.', 'LTRS.', 'L', 'L.', 'LITRE', 'LITRES', 'LT', 'LT.',\n",
    "                 \"KILOGRAMS\", 'KG', 'KG.', 'KGS', 'KGS.', 'KILO', 'KILO.', 'KILOS', 'KILOS.', 'KILOGRAM',\n",
    "                 \"INCHES\", 'INCH', 'IN.', 'INCEHES',\n",
    "                 \"FEET\", 'FT.', 'FT', 'FOOT',\n",
    "                 \"METRES\", 'METRE', 'METERS', 'METER', 'MT', 'MT.', 'MTS', 'MTS.', 'M', 'M.', 'MS', 'MS.', 'MTR',\n",
    "                 'MTRS',\n",
    "                 'CENTIMETRES', 'CM', 'CMS', 'CM.', 'CENTIMETER', 'CENTIMETRE', 'CENTIMETERS',\n",
    "                 'KILOMETRES', 'KM', 'KM.', 'KMS', 'KMS.', 'KILOMETERS', 'KILOMETER', 'KILOMETRE',\n",
    "                 'MILE', 'MI', 'MIS', 'MILES', 'MPG', 'MPGS', 'MI.', 'MPG.',\n",
    "                 'GALLON', 'GALLONS', 'GAL', 'GAL.', 'GALS', 'GALS.',\n",
    "                 'DOZEN', 'DZ', 'DZ.', 'DZS', 'DZS.', 'DOZ', 'DOZ.', 'DOZENS', 'DOZS',\n",
    "                 'QUART', 'QUARTS', 'QTS', 'QTS.',\n",
    "                 'PINT', 'PINTS',\n",
    "                 'CUP', 'CUPS',\n",
    "                 'WATTS', 'WATT', 'WTS', 'WTS.',\n",
    "                 'VOLTS', 'VOLTAGE', 'VOLT', 'V', 'V.', 'VT', 'VTS',\n",
    "                 'UNITS',\n",
    "                 'DEGREE', 'DEGREES',\n",
    "                 'KPL'])\n",
    "## For later use to parse unit by frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Creating an Dictionary of Unit Mapping\n",
    "unit_mapping_dict is a dictionary of lists.\n",
    "The units/abbreviations within a list will later be mapped to the dictionary key of said list (e.g. 'GMS.'-->'GRAMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_mapping_dict = {}\n",
    "unit_mapping_dict['GRAMS'] = ['GMS.', 'GMS', 'GM.', 'GRAM', ' GR ', ' GR.', 'GM', 'GRM', 'GRMS', 'GRM.', 'GRMS.']\n",
    "unit_mapping_dict['POUNDS'] = [' LB', 'LB', 'LB.', ' LBS', 'LBS', 'LBS.', 'POUND']\n",
    "unit_mapping_dict[\"OUNCES\"] = [\"OUNCES.\", 'OZ.', 'OZ', 'OUNCE', 'OUNCE.', 'OZS', 'OZS.']\n",
    "unit_mapping_dict[\"MILLILITRES\"] = ['MILLILITER', 'MILLILITRE', 'MILLILITERS', 'ML', 'ML.', 'MLS', 'MLS.', 'MIL',\n",
    "                                    'MIL.', 'CC', 'CC.', 'CCS', 'CCS.']\n",
    "unit_mapping_dict[\"MILLIMETRES\"] = ['MILLIMETRE', 'MILLIMETER', 'MILLIMITERS', 'MM', 'MMS', 'MM.', 'MMS.']\n",
    "unit_mapping_dict[\"KILOGRAMS\"] = ['KG', 'KG.', 'KGS', 'KGS.', 'KILO', 'KILO.', 'KILOS', 'KILOS.', 'KILOGRAM']\n",
    "unit_mapping_dict[\"LITRES\"] = ['LITERS', 'LITER', 'LITRE', 'LTR', 'LTRS', 'LTR.', 'LTRS.', 'L', 'L.', 'LITRE', 'LITRES',\n",
    "                               'LT', 'LT.']\n",
    "unit_mapping_dict[\"INCHES\"] = ['INCH', 'IN.', 'INCEHES']\n",
    "unit_mapping_dict[\"FEET\"] = ['FT.', 'FT', 'FOOT']\n",
    "unit_mapping_dict[\"METRES\"] = ['METRE', 'METERS', 'METER', 'MT', 'MT.', 'MTS', 'MTS.', 'M', 'M.', 'MS', 'MS.', 'MTR',\n",
    "                               'MTRS']\n",
    "unit_mapping_dict['CENTIMETRES'] = ['CM', 'CMS', 'CM.', 'CENTIMETER', 'CENTIMETRE', 'CENTIMETERS']\n",
    "unit_mapping_dict['MILE'] = ['MI', 'MIS', 'MILES', 'MI.']  # 'MPG','MPGS','MPG.'\n",
    "unit_mapping_dict['KILOMETRES'] = ['KM', 'KM.', 'KMS', 'KMS.', 'KILOMETERS', 'KILOMETER', 'KILOMETRE']\n",
    "unit_mapping_dict['GALLONS'] = ['GALLON', 'GAL', 'GAL.', 'GALS', 'GALS.']\n",
    "unit_mapping_dict['DOZEN'] = ['DZ', 'DZ.', 'DZS', 'DZS.', 'DOZ', 'DOZ.', 'DOZENS', 'DOZS']\n",
    "unit_mapping_dict['QUARTS'] = ['QUART', 'QTS', 'QTS.']\n",
    "unit_mapping_dict['PINTS'] = ['PINT']\n",
    "unit_mapping_dict['CUPS'] = ['CUP']\n",
    "unit_mapping_dict['WATTS'] = ['WATT', 'WTS', 'WTS.', 'WTTS']\n",
    "unit_mapping_dict['VOLTS'] = ['VOLTAGE', 'VOLT', 'V', 'V.', 'VT', 'VTS']\n",
    "\n",
    "\n",
    "#Creating unit_mapping_dict_2 needs to be done for the sake of our mapping operation\n",
    "\n",
    "unit_mapping_dict_2 = {}\n",
    "for item in unit_mapping_dict:\n",
    "    for entry in unit_mapping_dict[item]:\n",
    "        unit_mapping_dict_2[entry] = item\n",
    "        \n",
    "### MAKE UNIT CONVERSION EXHAUSTIVE LOOK THROUGH ALL SPEC CODES AND CULL OUT THE UNITS AND TABULATE ACROSS MEASUREMENT CATEGORIES.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Operation.\n",
    "\n",
    "This appears very complicated but that's mostly because there are a few different string cases to deal with:\n",
    "    (1) \"Briefcase: 10 X 15 X 20 CM\"\n",
    "    (2) \"Briefcase: 10 CM X 15 CM X 20 CM\"\n",
    "    (3) \"Briefcase: \"10 X 15 CM\" etc\n",
    "This operation searches strings for words that are in our unit_set (\"if\" statement at *****). If it finds a unit,\n",
    "    it then iterates (in reverse order) over the words in the string to try to find a quantity to go with the unit.\n",
    "\n",
    "note that it won't do anything if there's no unit (e.g. 'GM') in the string\n",
    "\n",
    "The bottom-level \"if\" and \"elif\" statements are for different arrangements of unit-measurements. For examples, see (1), (2), and (3) from a few lines above.\n",
    "    *note that these multiple cases only need to be dealt with in the case of looking for actual units like CM,\n",
    "        because multidimensional measurements are guaranteed to be measured in actual units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Actual_Units_Parsed_List = [[] for i in range(len(Unit))]\n",
    "for index, line in enumerate(new_split_Spec_code):\n",
    "    for ind, word in reversed(list(enumerate(line))):\n",
    "        # temp_list = []\n",
    "        if any(x.isdigit() for x in word) and ind + 1 in range(len(line)) and line[ind + 1] in unit_set and not (\n",
    "                ind + 2 in range(len(line)) and (line[ind + 2] == 'X' or line[ind + 2] == 'BY')): #********\n",
    "\n",
    "\n",
    "            if (ind - 1 in range(len(line)) and line[ind - 1] != 'X' and line[ind - 1] != 'BY') or ind - 1 not in range(\n",
    "                    len(line)):  # One dimensional case\n",
    "                Actual_Units_Parsed_List[index].append(' '.join(line[ind:ind + 2]))\n",
    "                new_split_Spec_code_for_removing_units[index] = new_split_Spec_code_for_removing_units[index][:ind] + \\\n",
    "                                                                new_split_Spec_code_for_removing_units[index][ind + 2:]\n",
    "\n",
    "            elif ind - 1 in range(len(line)) and line[ind - 1] == 'BY' or line[ind - 1] == 'X':\n",
    "\n",
    "                if ind - 6 in range(len(line)) and line[ind - 2] in unit_set and any(\n",
    "                        x.isdigit() for x in line[ind - 3]) and (line[ind - 4] == 'X' or line[ind - 4] == 'BY') and \\\n",
    "                        line[ind - 5] in unit_set and any(x.isdigit() for x in line[ind - 6]):  # 3-D case\n",
    "                    Actual_Units_Parsed_List[index].append(' '.join(line[ind - 6:ind + 2]))\n",
    "                    new_split_Spec_code_for_removing_units[index] = new_split_Spec_code_for_removing_units[index][\n",
    "                                                                    :ind - 6] + new_split_Spec_code_for_removing_units[\n",
    "                                                                                    index][ind + 2:]\n",
    "                    ## Each one has unit attached\n",
    "                    break\n",
    "                elif ind - 3 in range(len(line)) and line[ind - 2] in unit_set and any(\n",
    "                        x.isdigit() for x in line[ind - 3]):\n",
    "                    Actual_Units_Parsed_List[index].append(' '.join(line[ind - 3:ind + 2]))\n",
    "                    new_split_Spec_code_for_removing_units[index] = new_split_Spec_code_for_removing_units[index][\n",
    "                                                                    :ind - 3] + new_split_Spec_code_for_removing_units[\n",
    "                                                                                    index][ind + 2:]\n",
    "                    break\n",
    "                elif ind - 4 in range(len(line)) and any(x.isdigit() for x in line[ind - 2]) and (\n",
    "                        line[ind - 3] == 'X' or line[ind - 3] == 'BY') and any(x.isdigit() for x in line[ind - 4]):\n",
    "                    Actual_Units_Parsed_List[index].append(' '.join(line[ind - 4:ind + 2]))\n",
    "                    new_split_Spec_code_for_removing_units[index] = new_split_Spec_code_for_removing_units[index][\n",
    "                                                                    :ind - 4] + new_split_Spec_code_for_removing_units[\n",
    "                                                                                    index][ind + 2:]\n",
    "                    break\n",
    "                elif ind - 2 in range(len(line)) and any(x.isdigit() for x in line[ind - 2]):\n",
    "                    Actual_Units_Parsed_List[index].append(' '.join(line[ind - 2:ind + 2]))\n",
    "                    new_split_Spec_code_for_removing_units[index] = new_split_Spec_code_for_removing_units[index][\n",
    "                                                                    :ind - 2] + new_split_Spec_code_for_removing_units[\n",
    "                                                                                    index][ind + 2:]\n",
    "                    break\n",
    "\n",
    "Actual_Units_Parsed_List_2 = [', '.join(x) for x in Actual_Units_Parsed_List]\n",
    "\n",
    "for index, line in enumerate(new_split_Spec_code):\n",
    "    new_split_Spec_code[index] = ' '.join(new_split_Spec_code[index])\n",
    "\n",
    "for index, x in enumerate(new_split_Spec_code):\n",
    "    new_split_Spec_code[index] = x.split()\n",
    "#can we think of identifying the most common unit in each category and report that unit for all if available and iterate accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps units to common denotation (e.g. 'GMS' --> 'GRAMS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Actual_Units_Parsed_List_4 = ['' for i in range(len(Actual_Units_Parsed_List_2))]\n",
    "for index, line in enumerate(Actual_Units_Parsed_List_2):\n",
    "    for i, phrase in enumerate(line.split(',')):\n",
    "        temp_phrase = ''\n",
    "        for ind, word in enumerate(phrase.split()):\n",
    "            if word.strip() in unit_mapping_dict_2:\n",
    "                temp_phrase += unit_mapping_dict_2[word.strip()] + ' '\n",
    "            else:\n",
    "                temp_phrase += word + ' '\n",
    "        if len(Actual_Units_Parsed_List_4[index]) > 0:\n",
    "            Actual_Units_Parsed_List_4[index] += ', ' + temp_phrase\n",
    "        else:\n",
    "            Actual_Units_Parsed_List_4[index] += temp_phrase\n",
    "##Normanclatures are standardised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End of PARSING ACTUAL UNITS such as 'CM' direct from string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PARSING UNITS FROM GIVEN UNIT MEASUREMENTS.\n",
    "\n",
    "    In this section, unusual units of measurement are taken from the unit measurements that came with the data,\n",
    "    For example, the 7th observation comes with a unit description of \"10 Sticks\" ( Unit[6]; in \"AGARBATI/DHOOP\" category ).\n",
    "    This section of code takes \"STICKS\" and looks for instances of this unit in the AGARBATI/DHOOP category's descriptions (Spec_code)\n",
    "\n",
    "#### Creating Dictionary and List for Units from Specifications\n",
    "    Each itemcode category (e.g. \"AGARBATI/DHOOP\") has its own entry in Parsed_from_Given_Units_Dict.\n",
    "    Parsed_from_Given_Units_Dict[\"AGARBATI/DHOOP\"] == all units of measurement that are given in the \"Unit\" column of the data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Parsed_from_Given_Units_Dict = {}\n",
    "Parsed_from_Given_Units_List = [[] for i in range(len(Unit))]\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    Parsed_from_Given_Units_Dict[item_category] = set()\n",
    "    \"\"\"\n",
    "Adds words to dict if they:\n",
    "    1) != \"EACH\"\n",
    "    2) Are not in unit_set from above section (i.e. they are not real units of measurement like \"CM\")\n",
    "    3) Are not numerical digits\n",
    "\n",
    "    These 3 criteria are enforced on line 414 at *****\n",
    "\"\"\"\n",
    "for index, line in enumerate(split_Unit_code):\n",
    "    if len(line) != 0:\n",
    "        temp_list = []\n",
    "        for word in line:\n",
    "            if not word.isdigit() and not word.upper() in unit_set and word != 'Each':  #*****\n",
    "                temp_list.append(word.upper())\n",
    "        joined_list = ' '.join(temp_list)\n",
    "        Parsed_from_Given_Units_Dict[Itemcode[index]].add(joined_list)\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    if '' in Parsed_from_Given_Units_Dict[item_category]:\n",
    "        Parsed_from_Given_Units_Dict[item_category].remove('')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This parses a string for words that we have just declared to be units.\n",
    "\n",
    "For example, in the \"AGARBATI/DHOOP\" category, we found (lines 410-421) that \"STICKS\" is given to us as a unit of measurement.\n",
    "    Then, for every observation in the \"AGARBATI/DHOOP\" category, we will search the observation for a number followed by \"STICKS\".\n",
    "\n",
    "\"\"\"\n",
    "for index, line in enumerate(\n",
    "        new_split_Spec_code):\n",
    "    temp_list = []\n",
    "    for ind, word in enumerate(line):\n",
    "        if word.isdigit():\n",
    "            if ind + 1 in range(len(line)) and not (line[ind + 1] in unit_set) and line[ind + 1] in \\\n",
    "                    Parsed_from_Given_Units_Dict[Itemcode[index]]:\n",
    "                temp_list.append(' '.join(line[ind:ind + 2]))\n",
    "    Parsed_from_Given_Units_List[index] = ', '.join(temp_list)\n",
    "\n",
    "#Parsed_from_Given_Units_List contains the parsed units\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "End of parsing units such as 'STICKS' direct from GIVEN UNIT MEASUREMENTS. Lines ~ 400-470\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PARSING UNITS BY FREQUENCY\n",
    "    Note that this is BY FAR the least reliable of the 3 methods for parsing units (although it is still useful.\n",
    "    To improve accuracy, I limited this to item categories with at least 7 observations (note that 7 was an arbitrary choice that seemed to work well).\n",
    "    This threshold can be changed or removed by changing the \"IF\" statements at the beginning of several operations\n",
    "##### Setting up dictionaries and lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_unit_dict = {}\n",
    "parsed_unit_dict_sets = {}\n",
    "total_number_of_terms = {}\n",
    "parsed_unit_dict_for_checking = {}\n",
    "frequency_percentage = {}\n",
    "itemcode_set = set()\n",
    "intermediate_dict_of_dicts = {}\n",
    "length_of_itemcode = {}\n",
    "\n",
    "for item_category in Itemcode:\n",
    "    itemcode_set.add(item_category)\n",
    "    intermediate_dict_of_dicts[item_category] = {}\n",
    "    length_of_itemcode[item_category] = 0\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    parsed_unit_dict[item_category] = []\n",
    "    total_number_of_terms[item_category] = 0\n",
    "    parsed_unit_dict_for_checking[item_category] = []\n",
    "    parsed_unit_dict_sets[item_category] = set()\n",
    "    frequency_percentage[item_category] = {}\n",
    "\n",
    "for index, line in enumerate(new_split_Spec_code):\n",
    "    length_of_itemcode[Itemcode[index]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Take note/count of all words in a category, if they:\n",
    "    1) Follow a word comprised of digits (a number)\n",
    "    2) Are not themselves a word comprised of digits\n",
    "    3) Are not in unit_set (i.e. are not actual units like \"CM\")\n",
    "    4) Are not 'X', 'BY', 'SIZE', or 'IN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(new_split_Spec_code):\n",
    "    if length_of_itemcode[Itemcode[index]] >= 7:\n",
    "        for ind, word in enumerate(line):\n",
    "            if word.isdigit() and ind + 1 in range(len(line)) and not line[ind + 1].isdigit() and not line[\n",
    "                                                                                                          ind + 1] in unit_set and \\\n",
    "                    line[ind + 1] not in ('X', 'BY', 'SIZE',\n",
    "                                          'IN'):  # != 'X' and line[ind+1] != 'BY' and line[ind+1]: #and line[ind+1] not in stopwords:\n",
    "                parsed_unit_dict[Itemcode[index]].append(line[ind + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Deletes parsed potential units if they are not nouns (NN==Singular Noun, etc.)\n",
    "    More information on NLTK and this process can be found at\n",
    "        https://medium.com/@gianpaul.r/tokenization-and-parts-of-speech-pos-tagging-in-pythons-nltk-library-2d30f70af13b\n",
    "        or\n",
    "        https://www.nltk.org/book/ch05.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_category in itemcode_set:\n",
    "    if length_of_itemcode[item_category] >= 7:\n",
    "        for ind, word in enumerate(parsed_unit_dict[item_category]):\n",
    "            if not (pos_tag(word.lower().split())[0][1] in ('NN', 'NNS', 'NNP', 'NNPS')):\n",
    "                del parsed_unit_dict[item_category][ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This converts a raw count of a word's occurences within an itemcode category (e.g. AGARBATI/DHOOP) to a percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for item_category in itemcode_set:\n",
    "    if length_of_itemcode[item_category] >= 7:\n",
    "        total_number_of_terms[item_category] = len(parsed_unit_dict[item_category])\n",
    "        parsed_unit_dict_for_checking[item_category] = FreqDist(parsed_unit_dict[item_category]).most_common(\n",
    "            len(set(parsed_unit_dict[item_category])))\n",
    "        for int in range(len(parsed_unit_dict_for_checking[item_category])):\n",
    "            frequency_percentage[item_category][parsed_unit_dict_for_checking[item_category][int][0]] = \\\n",
    "                parsed_unit_dict_for_checking[item_category][int][1] / total_number_of_terms[item_category]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data structures necessary to combine units \n",
    "\n",
    "WITHIN an itemcode category [e.g. AGARBATI/DHOOP]) if they have a JaroWinkler similarity > 0.8\n",
    "This was done to combine things like \"CIGARETTES\" and \"CIGARETTE\"\n",
    "\n",
    "Note that their frequencies are also combined, so the frequency of the new combined word is accurate.\n",
    "\n",
    "intermediate_dict_of_dicts[Item Category][Potential Unit] holds all words that have JaroWinkler similarity > 0.8 to the Potential Unit.\n",
    "It contains the frequency of each word, NOT the JaroWinkler similarity between them.\n",
    "\n",
    "    For example, intermediate_dict_of_dicts[\"CIGARETTE\"] ==\n",
    "    {'CIGARETTES':\n",
    "    {\n",
    "        'CIGARETTES': 0.3763440860215054,\n",
    "        'CIGARETTE': 0.15053763440860216,\n",
    "        'CIGRETTE': 0.08602150537634409,\n",
    "        'CIGRETTES': 0.053763440860215055,\n",
    "        'CIGARETTERS': 0.010752688172043012,\n",
    "        'CIGARATTE': 0.010752688172043012,\n",
    "        'CIGRETE': 0.010752688172043012\n",
    "        },\n",
    "\n",
    "    CIGARETTE':\n",
    "    {'CIGARETTES': 0.3763440860215054,\n",
    "    ...\n",
    "    }\n",
    "\n",
    "    'SQUARE':\n",
    "    {\n",
    "        'SQUARE': 0.20430107526881722\n",
    "        },\n",
    "\n",
    "    'CIGRETTE': {..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item_category in itemcode_set:\n",
    "    if length_of_itemcode[item_category] >= 7:\n",
    "        for x in frequency_percentage[item_category]:\n",
    "            intermediate_dict_of_dicts[item_category][x] = {}\n",
    "            for y in frequency_percentage[item_category]:\n",
    "                if textdistance.jaro_winkler.normalized_similarity(x, y) > .8 and not (x == '' and y == ''):\n",
    "                    intermediate_dict_of_dicts[item_category][x][y] = (frequency_percentage[item_category][y])\n",
    "        for j in frequency_percentage[item_category]:\n",
    "            frequency_percentage[item_category][j] = sum(\n",
    "                intermediate_dict_of_dicts[item_category][j][x] for x in intermediate_dict_of_dicts[item_category][j])\n",
    "\n",
    "provisional_non_traditional_unit_list = [[] for i in range(len(Unit))]\n",
    "provisional_non_traditional_unit_list_2 = [[] for i in range(len(Unit))]\n",
    "provisional_non_traditional_unit_list_3 = [[] for i in range(len(Unit))]    #this will be the final list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Units of Measurement\n",
    "\n",
    "    This goes through every observation and find any words that we just decided are units of measurement.\n",
    "    It pulls a number (quantity) and word (unit) out of a string IF:\n",
    "        1) 1st word is a number\n",
    "        2) 2nd word is not a number\n",
    "        3) 2nd word is not in unit_set (i.e. not a real unit of measurement like CM\n",
    "        4) 2nd word has a frequency percentage of at least 0.3 (30%)\n",
    "                -means that this word accounts for at least 30% of words that follow numbers (in this itemcode category (e.g. AGARBATI/DHOOP)).\n",
    "                Note that 30% was chosen as an arbitrary threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, line in enumerate(new_split_Spec_code):\n",
    "    temp_list = []\n",
    "    for ind, word in enumerate(line):\n",
    "        if word.isdigit() and ind + 1 in range(len(line)) and not (line[ind + 1].isdigit()) and not (\n",
    "                line[ind + 1] in unit_set) and line[ind + 1] in frequency_percentage[Itemcode[index]] and \\\n",
    "                frequency_percentage[Itemcode[index]][line[ind + 1]] > .3:\n",
    "            temp_list.append(' '.join(line[ind:ind + 2]))\n",
    "    provisional_non_traditional_unit_list[index] = ' '.join(temp_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Unit and Quantity\n",
    "    This takes the most similar unit from of a group of potential units (e.g. taking 'CIGARETTES' out of ['CIGARETTES', 'CIGARETTE', 'CIGRETTE', 'CIGRETTES', 'CIGARETTERS', 'CIGARATTE', 'CIGRETE'])\n",
    "    and combines it with the quantity to create a full unit-measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, unit in enumerate(provisional_non_traditional_unit_list):\n",
    "    if len(unit) > 0:\n",
    "        for ind, word in enumerate(unit.split()):\n",
    "            # print(word)\n",
    "            if word.isdigit():\n",
    "                provisional_non_traditional_unit_list_2[index].append(word + ' ' + str(\n",
    "                    max(intermediate_dict_of_dicts[Itemcode[index]][unit.split()[ind + 1]],\n",
    "                        key=intermediate_dict_of_dicts[Itemcode[index]][unit.split()[\n",
    "                            ind + 1]].get)))\n",
    "\n",
    "provisional_non_traditional_unit_list_3 = [', '.join(x) for x in provisional_non_traditional_unit_list_2]\n",
    "\n",
    "\n",
    "new_units = [' '.join(x) for x in split_Unit_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of PARSING UNITS BY FREQUENCY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNIT CONVERSION\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_conversion_dict = {'METRES': ['CENTIMETRES', 100], 'METRE': ['CENTIMETRES', 100], 'METERS': ['CENTIMETRES', 100],\n",
    "                        'METER': ['CENTIMETRES', 100], 'MTR': ['CENTIMETRES', 100], 'MTRS': ['CENTIMETRES', 100],\n",
    "                        'MT': ['CENTIMETRES', 100], 'MILLIMETRES': ['CENTIMETRES', .1],\n",
    "                        'MILLIMETRE': ['CENTIMETRES', .1], 'MILLIMETERS': ['CENTIMETRES', .1],\n",
    "                        'MILLIMETER': ['CENTIMETRES', .1], 'CMS': ['CENTIMETRES', 1], 'CM': ['CENTIMETRES', 1],\n",
    "                        'CENTIMETRESS': ['CENTIMETRES', 1], 'MILE': ['KILOMETRES', 1.60934],\n",
    "                        'MILES': ['KILOMETRES', 1.60934], 'FEET': ['CENTIMETRES', 30.48],\n",
    "                        'FOOT': ['CENTIMETRES', 30.48], 'INCHES': ['CENTIMETRES', 2.54], 'KILOGRAMS': ['GRAMS', 1000],\n",
    "                        'KG': ['GRAMS', 1000], 'KGS': ['GRAMS', 1000], 'GM': ['GRAMS', 1], 'GMS': ['GRAMS', 1],\n",
    "                        'POUNDS': ['GRAMS', 453.592], 'DOZEN': ['UNITS', 12], 'QUARTS': ['MILLILITRES', 946.353],\n",
    "                        'LITRES': ['MILLILITRES', 1000], 'LITRE': ['MILLILITRES', 1000],'MILLILITRE': ['MILLILITRES', 1],\n",
    "                        'LITERS': ['MILLILITRES', 1000], 'LITER': ['MILLILITRES', 1000],\n",
    "                        'GALLON': ['MILLILITRES', 3785.412], 'ML': ['MILLILITRES', 1], 'UNITS': ['UNITS', 1],\n",
    "                        'DEGREE': ['DEGREE', 1]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates \"trivial\" entries in unit_mapping_dict, which map to themselves and are multiplied by 1 to be converted. DO NOT DELETE!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in unit_mapping_dict: #unit mapping dict was defined earlier for the first stage of parsing\n",
    "    if x not in unit_conversion_dict:\n",
    "        unit_conversion_dict[x] = [x, 1]\n",
    "\n",
    "\n",
    "\n",
    "split_Actual_Units_Parsed_List_4 = [[]] * len(Actual_Units_Parsed_List_4)\n",
    "for index, x in enumerate(Actual_Units_Parsed_List_4):\n",
    "    split_Actual_Units_Parsed_List_4[index] = x.split(',')\n",
    "\n",
    "for index, x in enumerate(split_Actual_Units_Parsed_List_4):\n",
    "    # print(x)\n",
    "    for ind, phrase in enumerate(x):\n",
    "        split_Actual_Units_Parsed_List_4[index][ind] = phrase.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coverting Units\n",
    "    This function converts units.\n",
    "\n",
    "    If all units came in the form \"12 INCHES\", this would be a simple task and the first IF statement would take care of it.\n",
    "    Unfortunately, there are units of the form\n",
    "        \"12 INCHES X 14 INCHES\"\n",
    "        \"12 X 14 INCHES\"\n",
    "        \"12 INCHES X 14 CENTIMETRES\"\n",
    "        etc.\n",
    "    so there is a lot to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phrase_converter_v2(string_list):  # for inputs that aren't pre-split\n",
    "\n",
    "    if len(string_list.split()) == 2 and all((x.isdigit() or x == '.') for x in string_list.split()[0]) and \\\n",
    "            string_list.split()[1] in unit_conversion_dict and len(unit_conversion_dict[string_list.split()[1]]) > 0:\n",
    "        return str(float(string_list.split()[0]) * unit_conversion_dict[string_list.split()[1]][1]) + ' ' + \\\n",
    "               unit_conversion_dict[string_list.split()[1]][0]\n",
    "\n",
    "    elif len(string_list.split()) > 2:  # all other cases start here\n",
    "\n",
    "        #   figuring out what kind of problem the phrase poses\n",
    "        count_of_numbers = 0\n",
    "        count_of_units = 0\n",
    "        for ind, word in enumerate(string_list.split()):\n",
    "            if all((letters.isdigit() or letters == '.') for letters in word):\n",
    "                count_of_numbers += 1\n",
    "            elif word in unit_conversion_dict:\n",
    "                count_of_units += 1\n",
    "\n",
    "        temp_string = ''\n",
    "\n",
    "        if count_of_numbers == count_of_units:  # if every number is followed by a unit:\n",
    "            for ind, word in enumerate(string_list.split()):\n",
    "                if all((letters.isdigit() or letters == '.') for letters in word) and ind + 1 in range(\n",
    "                        len(string_list.split())) and string_list.split()[ind + 1] in unit_conversion_dict:\n",
    "                    if len(unit_conversion_dict[string_list.split()[ind + 1]]) > 0:\n",
    "                        temp_string += \\\n",
    "                            str(round((float(string_list.split()[ind]) * unit_conversion_dict[string_list.split()[ind + 1]][\n",
    "                                1]), 3)) + ' ' + unit_conversion_dict[string_list.split()[ind + 1]][0] + ' '\n",
    "\n",
    "                    else:\n",
    "                        temp_string += string_list.split()[ind:ind + 2]\n",
    "                elif word in ('X', 'BY'):\n",
    "                    temp_string += word + ' '\n",
    "            return temp_string\n",
    "\n",
    "        elif count_of_numbers > count_of_units: #if there are less units than numbers\n",
    "\n",
    "            units = []\n",
    "            for words in string_list.split():\n",
    "                if not any(x.isdigit() for x in words):\n",
    "                    units.append(words)\n",
    "\n",
    "            unit_set = set()\n",
    "            for x in units:\n",
    "                if x not in ('X', 'BY'):\n",
    "                    unit_set.add(x)\n",
    "\n",
    "            temp_split_phrase = []\n",
    "\n",
    "            if len(unit_set) == 1:\n",
    "                for index, word in enumerate(string_list.split()):\n",
    "                    if all(x.isdigit() or x == '.' for x in word) and index+1 in range(len(string_list.split())) \\\n",
    "                            and string_list.split()[index+1] in ('X', 'BY'):\n",
    "                        temp_split_phrase.append(word)\n",
    "                        temp_split_phrase.append(min(unit_set))\n",
    "                    else:\n",
    "                        temp_split_phrase.append(word)\n",
    "                phrase_converter_v2(' '.join(temp_split_phrase))\n",
    "            else:\n",
    "                #print('EXAMINE THIS CASE:', string_list)\n",
    "                return string_list\n",
    "\n",
    "            return ' '.join(temp_split_phrase)\n",
    "\n",
    "        elif count_of_numbers < count_of_units:\n",
    "            return string_list\n",
    "    else:\n",
    "        return string_list\n",
    "\n",
    "\n",
    "del int\n",
    "intermediate_converted_parsed_unit_list = ['' for i in range(len(Actual_Units_Parsed_List_4))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Converting the 1st parsed list (of actual units like \"Centimetres\")\n",
    "\"\"\"\n",
    "for index, x in enumerate(Actual_Units_Parsed_List_4):\n",
    "    for ind, y in enumerate(x.split(',')):\n",
    "        if phrase_converter_v2(y) is not None:\n",
    "            if ind == 0:\n",
    "                intermediate_converted_parsed_unit_list[index] += phrase_converter_v2(y)\n",
    "            else:\n",
    "                intermediate_converted_parsed_unit_list[index] += ', ' + phrase_converter_v2(y)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Converting the 2nd parsed list (of units taken from the list that came with the data)\n",
    "\n",
    "    *note that there's no point in running the conversion process on the 3rd parsed list (frequent words),\n",
    "    since those are guaranteed to NOT be convertible.\n",
    "\"\"\"\n",
    "final_parsed_list = ['' for i in range(len(intermediate_converted_parsed_unit_list))]\n",
    "for index, line in enumerate(intermediate_converted_parsed_unit_list):\n",
    "    for ind, word in enumerate(line.split()):\n",
    "        if all(x.isdigit() or x == '.' for x in word):\n",
    "            if float(word) % 1 == 0:\n",
    "                final_parsed_list[index] += str(int(float(word))) + ' '\n",
    "            else:\n",
    "                final_parsed_list[index] += str(round(float(word),3)) + ' '\n",
    "        else:\n",
    "            final_parsed_list[index] += word + ' '\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Converting the original unit list (that came with the data)\n",
    "\"\"\"\n",
    "intermediate_converted_unit_list = ['' for i in range(len(new_units))]\n",
    "for index, x in enumerate(new_units):\n",
    "    for ind, y in enumerate(x.upper().split(',')):\n",
    "        if phrase_converter_v2(y) is not None:\n",
    "            if ind == 0:\n",
    "                intermediate_converted_unit_list[index] += phrase_converter_v2(y)\n",
    "            else:\n",
    "                intermediate_converted_unit_list[index] += ', ' + phrase_converter_v2(y)\n",
    "\n",
    "final_original_unit_list = ['' for i in range(len(intermediate_converted_unit_list))]\n",
    "for index, line in enumerate(intermediate_converted_unit_list):\n",
    "    for ind, word in enumerate(line.split()):\n",
    "        if all(x.isdigit() or x == '.' for x in word):\n",
    "            if float(word) % 1 == 0:\n",
    "                final_original_unit_list[index] += str(int(float(word))) + ' '\n",
    "            else:\n",
    "                final_original_unit_list[index] += str(round(float(word), 3)) + ' '\n",
    "        else:\n",
    "            final_original_unit_list[index] += word + ' '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cleaning up outliers that for some reason aren't converted\n",
    "\n",
    "Feb 12\n",
    "\"\"\"\n",
    "\n",
    "for i, line in enumerate(final_original_unit_list):     #2nd sweep\n",
    "    \n",
    "    if \"MILLIMETRES\" in line or \"INCHES\" in line:\n",
    "        final_original_unit_list[i] = phrase_converter_v2(line)\n",
    "        \n",
    "    if \"MILLIMETRES\" in final_parsed_list[i] or \"INCHES\" in final_parsed_list[i]:\n",
    "        final_parsed_list[i] = phrase_converter_v2(final_parsed_list[i])\n",
    "\n",
    "    if \"MILLIMETRES\" in Parsed_from_Given_Units_List[i] or \"INCHES\" in Parsed_from_Given_Units_List[i]:\n",
    "        Parsed_from_Given_Units_List[i] = phrase_converter_v2(Parsed_from_Given_Units_List[i])\n",
    "\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "3rd sweep: going for multi-dimensional & multiple measure-observations. These are only found in final_parsed_list\n",
    "\"\"\"\n",
    "        \n",
    "for i, line in enumerate(final_parsed_list):      \n",
    "    \n",
    "    if \",\" in line:\n",
    "        temp = []\n",
    "        for x in line.split(\",\"):\n",
    "            temp.append(phrase_converter_v2(x))\n",
    "        final_parsed_list[i] = \", \".join(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of UNIT CONVERSION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling a FINAL UNIT LIST from each of the individual lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FINAL_UNITS = [0] * len(Unit)\n",
    "\n",
    "\n",
    "def final_unit_compiler():\n",
    "    for index, u in enumerate(final_original_unit_list):\n",
    "        if '1 EACH' not in u and '1 PER' not in u:\n",
    "            FINAL_UNITS[index] = u\n",
    "    for ind, t in enumerate(FINAL_UNITS):\n",
    "        if t == 0 and len(final_parsed_list[ind]) > 0:  # and len((Actual_Units_Parsed_List_2[ind]).split()) < 3:\n",
    "            FINAL_UNITS[ind] = final_parsed_list[ind]\n",
    "    for i, element in enumerate(FINAL_UNITS):\n",
    "        if element == 0:\n",
    "            if len(Parsed_from_Given_Units_List[i]) > 0:\n",
    "                FINAL_UNITS[i] = Parsed_from_Given_Units_List[i]\n",
    "    for i, cosa in enumerate(FINAL_UNITS):\n",
    "        if cosa == 0:\n",
    "            if provisional_non_traditional_unit_list[i] is not None:\n",
    "                if len(provisional_non_traditional_unit_list_3[i]) > 0:\n",
    "                    FINAL_UNITS[i] = provisional_non_traditional_unit_list_3[i]\n",
    "    for index, x in enumerate(FINAL_UNITS):\n",
    "        if x == 0:\n",
    "            FINAL_UNITS[index] = '1 EACH'\n",
    "\n",
    "\n",
    "final_unit_compiler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLEANING DESCRIPTIONS\n",
    "In this section, descriptions (Spec_code) are cleaned up for JaroWinkler matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Part of Speech - tagged Spec_code\n",
    "new_pos_tagged_Spec_code = [pos_tag(x.lower().split()) for x in\n",
    "                            new_Spec_code]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Removes substrings from descriptions if they are structured as \"BUNDLE OF 50\", \"PACKET OF 10\", etc.\n",
    "    (i.e. NOUN, PREPOSITION, NUMBER)\n",
    "\"\"\"\n",
    "for index, line in enumerate(new_pos_tagged_Spec_code):\n",
    "    for ind, word in enumerate(line):\n",
    "        if word[1] in ('NN' or 'NNS' or 'NNP' or 'NNPS') and ind + 2 in range(len(line)) \\\n",
    "                and line[ind + 1][1] == 'IN' and (line[ind + 2][1] == 'CD' or line[ind + 2][0].isdigit()) and 'X' not in \\\n",
    "                new_split_Spec_code[index]:\n",
    "            new_pos_tagged_Spec_code[index] = new_pos_tagged_Spec_code[index][:ind] + new_pos_tagged_Spec_code[index][\n",
    "                                                                                      ind + 3:]\n",
    "\n",
    "for index, line in enumerate(new_pos_tagged_Spec_code):\n",
    "    new_Spec_code[index] = ' '\n",
    "    for ind, word in enumerate(line):\n",
    "        new_Spec_code[index] += word[0] + ' '\n",
    "\n",
    "\"\"\"\n",
    "Removing Units (quantity and unit of measurement) from new_Spec_code\n",
    "\"\"\"\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    if len(Actual_Units_Parsed_List[index]) > 0:\n",
    "        for units in Actual_Units_Parsed_List[index]:\n",
    "            for word in units.split():\n",
    "                new_Spec_code[index] = new_Spec_code[index].replace(word.lower(), ' ')\n",
    "    if len(Parsed_from_Given_Units_List[index]) > 0:\n",
    "        for x in Parsed_from_Given_Units_List[index].split():\n",
    "            new_Spec_code[index] = new_Spec_code[index].replace(x, ' ')\n",
    "    if len(provisional_non_traditional_unit_list[index]) > 0:\n",
    "        for y in provisional_non_traditional_unit_list[index].split():\n",
    "            new_Spec_code[index] = new_Spec_code[index].replace(y.lower(), ' ')\n",
    "\n",
    "for index, line in enumerate(new_Spec_code):  # removes multiple spaces\n",
    "    new_Spec_code[index] = ' '.join(line.split())\n",
    "\n",
    "\"\"\"\n",
    "Removing common words that do not contain identifying/unique information\n",
    "\"\"\"\n",
    "for index, x in enumerate(new_Spec_code):\n",
    "    new_Spec_code[index] = x.replace('medium', ' ').replace(' size ', ' ').replace('fresh', ' ').replace(\n",
    "        'quality', ' ').replace(' for ', ' ').replace(' with ', ' ').replace(' of ', ' ').replace(' and ',\n",
    "                                                                                                    ' ').replace(' to ',\n",
    "                                                                                                                 ' ').replace(\n",
    "        'without', ' ').replace(' other ', ' ').replace('pkt', ' ').replace('bundle', ' ').replace('pieces',\n",
    "                                                                                                         ' ').replace(\n",
    "        'including', ' ').replace(' made ', ' ')\n",
    "\n",
    "\n",
    "for index,line in enumerate(new_Spec_code):\n",
    "    new_Spec_code[index] = \" \".join(line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### END OF CLEANING DESCRIPTIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BEGINNING OF JAROWINKLER MATCHING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis code has been implemented further down\\n'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code has been implemented further down\n",
    "\"\"\"\n",
    "\n",
    "# first_spec_of_category = {'ACID PHENYL':0}\n",
    "# for index, line in enumerate(new_Spec_code):\n",
    "#     try:\n",
    "#         if Itemcode[index] != Itemcode[index + 1]:\n",
    "#             first_spec_of_category[Itemcode[index + 1]] = index + 1\n",
    "#     except:\n",
    "#         pass  \n",
    "    \n",
    "# itemcode_specs = {}\n",
    "# for item_category in itemcode_set:\n",
    "#     itemcode_specs[item_category] = []\n",
    "# for index, line in enumerate(new_Spec_code):\n",
    "#     itemcode_specs[Itemcode[index]].append((line, index))\n",
    "    \n",
    "# grouped_entries = {}    \n",
    "# for item_category in itemcode_set:\n",
    "#     grouped_entries[item_category] = []\n",
    "\n",
    "# itemcode_array_dict = {}\n",
    "# total_matching_entries_dict = {}\n",
    "# indices_dict = {}\n",
    "# local_grouped_entries_dict = {}\n",
    "\n",
    "# threshold = 0.8\n",
    "\n",
    "# for item_category in itemcode_set:\n",
    "#     itemcode_array_dict[item_category] = np.zeros((length_of_itemcode[item_category],length_of_itemcode[item_category]))\n",
    "#     total_matching_entries_dict[item_category] = np.zeros(length_of_itemcode[item_category])\n",
    "#     indices_dict[item_category] = [x + first_spec_of_category[item_category] for x in range(length_of_itemcode[item_category])]\n",
    "#     local_grouped_entries_dict[item_category] = []\n",
    "    \n",
    "#     for (x,y), value in np.ndenumerate(itemcode_array_dict[item_category]): #assign values to jaroW matrices\n",
    "#         itemcode_array_dict[item_category][x][y] = textdistance.jaro_winkler.similarity(itemcode_specs[item_category][x][0], itemcode_specs[item_category][y][0])\n",
    "    \n",
    "#     for i, x in enumerate(total_matching_entries_dict[item_category]):  #count entries>threshold per col\n",
    "#         for y in itemcode_array_dict[item_category][:,i]:\n",
    "#             if y > threshold:\n",
    "#                 total_matching_entries_dict[item_category][i] += 1\n",
    "\n",
    "# hard_set = set()\n",
    "    \n",
    "\n",
    "# def jaro_iteration(local_item_category, itemcode_array, indices, local_grouped_entries, local_threshold):\n",
    "#     print(local_threshold)\n",
    "#     print(itemcode_array)\n",
    "#     print(indices)\n",
    "#     print(\"local_grouped_entries = \", local_grouped_entries)\n",
    "#     print('\\n')\n",
    "    \n",
    "#     hard_set.add(local_item_category)\n",
    "#     total_matching_entries = np.zeros(length_of_itemcode[local_item_category])    \n",
    "#     for i, x in enumerate(total_matching_entries):  #count entries>threshold per col\n",
    "#             for y in itemcode_array_dict[local_item_category][:,i]:\n",
    "#                 if y > local_threshold:\n",
    "#                     total_matching_entries[i] += 1    \n",
    "#     print(total_matching_entries)\n",
    "\n",
    "\n",
    "#     max_index = np.argmax(total_matching_entries)\n",
    "#     temp_list = []\n",
    "\n",
    "#     for i, y in enumerate(itemcode_array[:,max_index]):\n",
    "#         if y>=local_threshold:\n",
    "#             temp_list.append(i + first_spec_of_category[local_item_category])\n",
    "#             print('matching entries = ', max_index + first_spec_of_category[local_item_category], i+ first_spec_of_category[local_item_category])\n",
    "#     local_grouped_entries.append(temp_list)\n",
    "\n",
    "#     for y in temp_list:\n",
    "#         itemcode_array[:, y - first_spec_of_category[local_item_category]] = 0\n",
    "#         itemcode_array[y - first_spec_of_category[local_item_category]] = 0\n",
    "\n",
    "\n",
    "#     total_matching_entries_dict = np.zeros(length_of_itemcode[local_item_category])\n",
    "#     for i, x in enumerate(total_matching_entries):\n",
    "#         for y in itemcode_array[:,i]:\n",
    "#             if y > local_threshold:\n",
    "#                 total_matching_entries[i] += 1\n",
    "    \n",
    "#     if any(x > 0 for x in total_matching_entries):    \n",
    "#         jaro_iteration(local_item_category, itemcode_array, indices, local_grouped_entries, local_threshold)\n",
    "#     else:    \n",
    "#         Spec_buckets[local_item_category] = []\n",
    "#         for x in local_grouped_entries:\n",
    "#             temp_set = set()\n",
    "#             for indices in x:\n",
    "#                 temp_set.add((new_Spec_code[indices], indices))\n",
    "#             Spec_buckets[local_item_category].append(temp_set)\n",
    "            \n",
    "\n",
    "    \n",
    "# jaro_iteration('ALMOND', itemcode_array_dict['ALMOND'], indices_dict['ALMOND'], local_grouped_entries_dict['ALMOND'], threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below sets up Jaro-Winkler comparisons  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-9ddc934cc2c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndenumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemcode_array_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#assign values to jaroW matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mitemcode_array_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtextdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaro_winkler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitemcode_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitemcode_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_matching_entries_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem_category\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m#count entries>threshold per col\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/textdistance/algorithms/base.py\u001b[0m in \u001b[0;36msimilarity\u001b[0;34m(self, *sequences)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mquick_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/textdistance/algorithms/edit_based.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, s1, s2, prefix_weight)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquick_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/textdistance/algorithms/base.py\u001b[0m in \u001b[0;36mquick_answer\u001b[0;34m(self, *sequences)\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0;31m# try get answer from external libs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/textdistance/algorithms/base.py\u001b[0m in \u001b[0;36mexternal_answer\u001b[0;34m(self, *sequences)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# try to get external libs for algorithm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mlibs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibraries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_libs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mlib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlibs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0;31m# if conditions not satisfied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_conditions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "ID_list = [-2] * len(Spec_code) #-2 is just a placeholder so we know if anything is wrong\n",
    "Spec_buckets = {}\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    Spec_buckets[item_category] = []\n",
    "\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    Spec_buckets[Itemcode[index]].append(set([(line, index)]))\n",
    "\n",
    "#above was here for soft jaro ____________________________________________________________________________________\n",
    "    \n",
    "#beginning of hard jaro set up _____________________________________________________________________________________    \n",
    "first_spec_of_category = {'ACID PHENYL':0}\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    try:\n",
    "        if Itemcode[index] != Itemcode[index + 1]:\n",
    "            first_spec_of_category[Itemcode[index + 1]] = index + 1\n",
    "    except:\n",
    "        pass  \n",
    "    \n",
    "itemcode_specs = {}\n",
    "for item_category in itemcode_set:\n",
    "    itemcode_specs[item_category] = []\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    itemcode_specs[Itemcode[index]].append((line, index))\n",
    "    \n",
    "grouped_entries = {}    \n",
    "for item_category in itemcode_set:\n",
    "    grouped_entries[item_category] = []\n",
    "\n",
    "itemcode_array_dict = {}\n",
    "total_matching_entries_dict = {}\n",
    "indices_dict = {}\n",
    "local_grouped_entries_dict = {}\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    itemcode_array_dict[item_category] = np.zeros((length_of_itemcode[item_category],length_of_itemcode[item_category]))\n",
    "    total_matching_entries_dict[item_category] = np.zeros(length_of_itemcode[item_category])\n",
    "    indices_dict[item_category] = [x + first_spec_of_category[item_category] for x in range(length_of_itemcode[item_category])]\n",
    "    local_grouped_entries_dict[item_category] = []\n",
    "    \n",
    "    for (x,y), value in np.ndenumerate(itemcode_array_dict[item_category]): #assign values to jaroW matrices\n",
    "        itemcode_array_dict[item_category][x][y] = textdistance.jaro_winkler.similarity(itemcode_specs[item_category][x][0], itemcode_specs[item_category][y][0])\n",
    "    \n",
    "    for i, x in enumerate(total_matching_entries_dict[item_category]):  #count entries>threshold per col\n",
    "        for y in itemcode_array_dict[item_category][:,i]:\n",
    "            if y > threshold:\n",
    "                total_matching_entries_dict[item_category][i] += 1\n",
    "\n",
    "hard_set = set()\n",
    "    \n",
    "#end of hard jaro set up _________________________________________________________________________________________________________________    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    2\n",
    "ID_list2 = [-2] * len(Spec_code) #-2 is just a placeholder so we know if anything is wrong\n",
    "Spec_buckets2 = {}\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    Spec_buckets2[item_category] = []\n",
    "\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    Spec_buckets2[Itemcode[index]].append(set([(line, index)]))\n",
    "\n",
    "#above was here for soft jaro ____________________________________________________________________________________\n",
    "    \n",
    "#beginning of hard jaro set up _____________________________________________________________________________________    \n",
    "grouped_entries = {}    \n",
    "for item_category in itemcode_set:\n",
    "    grouped_entries[item_category] = []\n",
    "\n",
    "itemcode_array_dict2 = {}\n",
    "total_matching_entries_dict2 = {}\n",
    "indices_dict2 = {}\n",
    "local_grouped_entries_dict2 = {}\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    itemcode_array_dict2[item_category] = np.zeros((length_of_itemcode[item_category],length_of_itemcode[item_category]))\n",
    "    total_matching_entries_dict2[item_category] = np.zeros(length_of_itemcode[item_category])\n",
    "    indices_dict2[item_category] = [x + first_spec_of_category[item_category] for x in range(length_of_itemcode[item_category])]\n",
    "    local_grouped_entries_dict2[item_category] = []\n",
    "    \n",
    "    for (x,y), value in np.ndenumerate(itemcode_array_dict2[item_category]): #assign values to jaroW matrices\n",
    "        itemcode_array_dict2[item_category][x][y] = textdistance.jaro_winkler.similarity(itemcode_specs[item_category][x][0], itemcode_specs[item_category][y][0])\n",
    "    \n",
    "    for i, x in enumerate(total_matching_entries_dict2[item_category]):  #count entries>threshold per col\n",
    "        for y in itemcode_array_dict2[item_category][:,i]:\n",
    "            if y > threshold:\n",
    "                total_matching_entries_dict2[item_category][i] += 1\n",
    "\n",
    "hard_set2 = set()\n",
    "    \n",
    "#end of hard jaro set up _________________________________________________________________________________________________________________    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     3\n",
    "\n",
    "ID_list3 = [-2] * len(Spec_code) #-2 is just a placeholder so we know if anything is wrong\n",
    "Spec_buckets3 = {}\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    Spec_buckets3[item_category] = []\n",
    "\n",
    "for index, line in enumerate(new_Spec_code):\n",
    "    Spec_buckets3[Itemcode[index]].append(set([(line, index)]))\n",
    "\n",
    "#above was here for soft jaro ____________________________________________________________________________________\n",
    "    \n",
    "#beginning of hard jaro set up _____________________________________________________________________________________    \n",
    "\n",
    "grouped_entries = {}    \n",
    "for item_category in itemcode_set:\n",
    "    grouped_entries[item_category] = []\n",
    "\n",
    "itemcode_array_dict3 = {}\n",
    "total_matching_entries_dict3 = {}\n",
    "indices_dict3 = {}\n",
    "local_grouped_entries_dict3 = {}\n",
    "\n",
    "threshold = 0.8\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    itemcode_array_dict3[item_category] = np.zeros((length_of_itemcode[item_category],length_of_itemcode[item_category]))\n",
    "    total_matching_entries_dict3[item_category] = np.zeros(length_of_itemcode[item_category])\n",
    "    indices_dict3[item_category] = [x + first_spec_of_category[item_category] for x in range(length_of_itemcode[item_category])]\n",
    "    local_grouped_entries_dict3[item_category] = []\n",
    "    \n",
    "    for (x,y), value in np.ndenumerate(itemcode_array_dict3[item_category]): #assign values to jaroW matrices\n",
    "        itemcode_array_dict3[item_category][x][y] = textdistance.jaro_winkler.similarity(itemcode_specs[item_category][x][0], itemcode_specs[item_category][y][0])\n",
    "    \n",
    "    for i, x in enumerate(total_matching_entries_dict3[item_category]):  #count entries>threshold per col\n",
    "        for y in itemcode_array_dict3[item_category][:,i]:\n",
    "            if y > threshold:\n",
    "                total_matching_entries_dict3[item_category][i] += 1\n",
    "\n",
    "hard_set3 = set()\n",
    "    \n",
    "#end of hard jaro set up _________________________________________________________________________________________________________________    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hard and Soft Jaro Comparison Algorithms\n",
    "\"\"\"\n",
    "\n",
    "def hard_jaro_comparison(local_item_category, itemcode_array, indices, local_grouped_entries, local_threshold):\n",
    "#     print(local_threshold)\n",
    "#     print(itemcode_array)\n",
    "#     print(indices)\n",
    "#     print(\"local_grouped_entries = \", local_grouped_entries)\n",
    "#     print('\\n')\n",
    "    \n",
    "#     hard_set.add(local_item_category)\n",
    "    total_matching_entries = np.zeros(length_of_itemcode[local_item_category])    \n",
    "    for i, x in enumerate(total_matching_entries):  #count entries>threshold per col\n",
    "            for y in itemcode_array_dict[local_item_category][:,i]:      #is this col or row?\n",
    "                if y > local_threshold:\n",
    "                    total_matching_entries[i] += 1    \n",
    "#     print(total_matching_entries)\n",
    "\n",
    "\n",
    "    max_index = np.argmax(total_matching_entries)\n",
    "    temp_list = []\n",
    "\n",
    "    for i, y in enumerate(itemcode_array[:,max_index]):\n",
    "        if y>=local_threshold:\n",
    "            temp_list.append(i + first_spec_of_category[local_item_category])\n",
    "#             print('matching entries = ', max_index + first_spec_of_category[local_item_category], i+ first_spec_of_category[local_item_category])\n",
    "    local_grouped_entries.append(temp_list)\n",
    "\n",
    "    for y in temp_list:\n",
    "        itemcode_array[:, y - first_spec_of_category[local_item_category]] = 0\n",
    "        itemcode_array[y - first_spec_of_category[local_item_category]] = 0\n",
    "\n",
    "\n",
    "    total_matching_entries_dict = np.zeros(length_of_itemcode[local_item_category])\n",
    "    for i, x in enumerate(total_matching_entries):\n",
    "        for y in itemcode_array[:,i]:\n",
    "            if y > local_threshold:\n",
    "                total_matching_entries[i] += 1\n",
    "    \n",
    "    #delete later - following paragraph is just for printing\n",
    "#     for x in local_grouped_entries:\n",
    "#         for y in x:\n",
    "#             print(y, new_Spec_code[y])\n",
    "#             for z in x:\n",
    "#                 print(y,z,textdistance.jaro_winkler.similarity(new_Spec_code[z],new_Spec_code[y]))\n",
    "    \n",
    "    if any(x > 0 for x in total_matching_entries):    \n",
    "        hard_jaro_comparison(local_item_category, itemcode_array, indices, local_grouped_entries, local_threshold)\n",
    "    else:    \n",
    "        Spec_buckets[local_item_category] = []\n",
    "        for x in local_grouped_entries:\n",
    "            temp_set = set()\n",
    "            for indices in x:\n",
    "                temp_set.add((new_Spec_code[indices], indices))\n",
    "            Spec_buckets[local_item_category].append(temp_set)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    2: soft\n",
    "def soft_jaro_comparison1(item_category, threshold):\n",
    "    for index, x in enumerate(Spec_buckets2[item_category]):\n",
    "        for ind, y in enumerate(Spec_buckets2[item_category]):\n",
    "            if ind != index:\n",
    "                if any([textdistance.jaro_winkler.normalized_similarity(a[0], b[0]) > threshold for a in x for b in y]):\n",
    "                    temp = Spec_buckets2[item_category][ind].union(Spec_buckets2[item_category][index])\n",
    "                    Spec_buckets2[item_category][ind] = temp\n",
    "                    Spec_buckets2[item_category][index] = []  # = temp\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    3: medium\n",
    "def soft_jaro_comparison2(item_category, threshold):\n",
    "    for index, x in enumerate(Spec_buckets3[item_category]):\n",
    "        for ind, y in enumerate(Spec_buckets3[item_category]):\n",
    "            if ind != index:\n",
    "                if all([textdistance.jaro_winkler.normalized_similarity(a[0], b[0]) > threshold for a in x for b in y]) and x != [] and y != []: #difference is here\n",
    "#                     print(Spec_buckets3[item_category][ind])\n",
    "#                     print(Spec_buckets3[item_category][index], '\\n')\n",
    "                    temp = Spec_buckets3[item_category][ind].union(Spec_buckets3[item_category][index])\n",
    "                    Spec_buckets3[item_category][ind] = temp\n",
    "                    Spec_buckets3[item_category][index] = []  # = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_jaro_comparison1('AGARBATI/DHOOP', .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spec_buckets['AGARBATI/DHOOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soft_jaro_comparison2('AGARBATI/DHOOP', .8)\n",
    "Spec_buckets['AGARBATI/DHOOP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#****\n",
    "for item_category in itemcode_set:\n",
    "    hard_jaro_comparison(item_category, itemcode_array_dict[item_category], indices_dict[item_category], grouped_entries[item_category], .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "for item_category in itemcode_set:    \n",
    "    soft_jaro_comparison1(item_category, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for item_category in itemcode_set:\n",
    "    soft_jaro_comparison2(item_category, .8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_grouped_entries_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assigning ID's\n",
    "\"\"\"\n",
    "for item_category in itemcode_set:\n",
    "    ID = 0\n",
    "    for x in Spec_buckets[item_category]:\n",
    "        if len(x) > 0:\n",
    "            for y in x:\n",
    "                ID_list[y[1]] = ID\n",
    "            ID += 1\n",
    "\n",
    "add = 0\n",
    "for index in range(len(ID_list)):\n",
    "    ID_list[index] += add\n",
    "    if index < len(ID_list) - 1 and Itemcode[index] != Itemcode[index + 1]:\n",
    "        add = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assigning ID's 2\n",
    "\"\"\"\n",
    "\n",
    "for item_category in itemcode_set:\n",
    "    ID = 0\n",
    "    for x in Spec_buckets[item_category]:\n",
    "        if len(x) > 0:\n",
    "            for y in x:\n",
    "                ID_list2[y[1]] = ID\n",
    "            ID += 1\n",
    "\n",
    "add = 0\n",
    "for index in range(len(ID_list2)):\n",
    "    ID_list2[index] += add\n",
    "    if index < len(ID_list2) - 1 and Itemcode[index] != Itemcode[index + 1]:\n",
    "        add = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assigning ID's 3\n",
    "\"\"\"\n",
    "for item_category in itemcode_set:\n",
    "    ID = 0\n",
    "    for x in Spec_buckets[item_category]:\n",
    "        if len(x) > 0:\n",
    "            for y in x:\n",
    "                ID_list3[y[1]] = ID\n",
    "            ID += 1\n",
    "\n",
    "add = 0\n",
    "for index in range(len(ID_list3)):\n",
    "    ID_list3[index] += add\n",
    "    if index < len(ID_list3) - 1 and Itemcode[index] != Itemcode[index + 1]:\n",
    "        add = index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_order_index = list(range(len(Spec_code)))\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Hard Jaro\": ID_list,\n",
    "    \"Medium Jaro\": ID_list3,\n",
    "    \"Soft Jaro\": ID_list2,\n",
    "    'Original Spec Code': Spec_code,\n",
    "    'New Spec Code': new_Spec_code,\n",
    "    'Final Unit List': FINAL_UNITS,\n",
    "    'Converted Original Units': final_original_unit_list,\n",
    "    'Parsed Actual Units': final_parsed_list,\n",
    "    'Parsed from Given Units': Parsed_from_Given_Units_List,\n",
    "    'Non-traditional units': provisional_non_traditional_unit_list_3,\n",
    "    'Itemcode': Itemcode,\n",
    "    'Original Order Index': original_order_index\n",
    "})\n",
    "end_time = time.time()\n",
    "print(\"Elapsed time was %g seconds\" % (end_time - start_time))\n",
    "# df.to_csv('Stratified Final Analysis 4 - <20 @soft 80%; <40@ soft 84%; >40 @hard 80%.csv', index=False)\n",
    "\n",
    "#######################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('Stratified Final Analysis 4.3 - <20 @soft 80%; <40@ soft 84%; >40 @hard 80% - localnames mapped.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
